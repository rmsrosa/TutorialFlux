{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--HEADER-->\n",
    "*Tutorial sobre Flux.jl - 2020/2 [- Ricardo M. S. Rosa (IM/UFRJ)](http://www.im.ufrj.br/rrosa)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--BADGES-->\n",
    "<a href=\"https://nbviewer.jupyter.org/github/rmsrosa/TutorialFlux/blob/main/notebooks/06.00-Algoritmos_treinamento.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://img.shields.io/badge/view%20in-nbviewer-orange\" alt=\"View in NBViewer\" title=\"View in NBViewer\"></a><a href=\"https://mybinder.org/v2/gh/rmsrosa/TutorialFlux/julia-env-for-binder?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Frmsrosa%252FTutorialFlux%26branch%3Dmain%26urlpath%3Dtree%252FTutorialFlux%252Fnotebooks/06.00-Algoritmos_treinamento.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in binder\" title=\"Open in binder\"></a><a href=\"https://nbviewer.jupyter.org/github/rmsrosa/TutorialFlux/blob/main/notebooks/slides/06.00-Algoritmos_treinamento.slides.html\" target=\"_blank\"><img align=\"left\" src=\"https://img.shields.io/badge/view-slides-darkgreen\" alt=\"View Slides\" title=\"View Slides\"></a>&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--NAVIGATOR-->\n",
    "[<- 5. Treinando redes neurais](05.00-Treinando_redes_neurais.ipynb) | [Página inicial](00.00-Pagina_inicial.ipynb) \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algoritmos de treinamento\n",
    "\n",
    "* O que o `Flux.train!` faz é apenas executar um passo de um dado algoritmo de otimização.\n",
    "\n",
    "* Vamos ver detalhes do `Flux.train!`.\n",
    "\n",
    "* Assim como as opções de algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Random\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Flux.train!`\n",
    "\n",
    "* O `Flux.train!` é parte do módulo [Flux.Optimise](https://github.com/FluxML/Flux.jl/blob/master/src/optimise/Optimise.jl).\n",
    "\n",
    "* Vejamos o [código do `Flux.train!`](https://github.com/FluxML/Flux.jl/blob/master/src/optimise/train.jl#L96):\n",
    "\n",
    "```julia\n",
    "function train!(loss, ps, data, opt; cb = () -> ())\n",
    "  ps = Params(ps)\n",
    "  cb = runall(cb)\n",
    "  @progress for d in data\n",
    "    try\n",
    "      gs = gradient(ps) do\n",
    "        loss(batchmemaybe(d)...)\n",
    "      end\n",
    "      update!(opt, ps, gs)\n",
    "      cb()\n",
    "    catch ex\n",
    "      if ex isa StopException\n",
    "        break\n",
    "      elseif ex isa SkipException\n",
    "        continue\n",
    "      else\n",
    "        rethrow(ex)\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detalhes\n",
    "\n",
    "* [Params](https://github.com/FluxML/Zygote.jl/blob/master/src/compiler/interface.jl#L67) e [gradient](https://github.com/FluxML/Zygote.jl/blob/master/src/compiler/interface.jl#L57) são importados do [FluxML/Zygote.jl](https://github.com/FluxML/Zygote.jl), que é um pacote de **diferenciação automática**.\n",
    "\n",
    "* `Params` é um tipo composto que guarda quais parâmetros e em que ordem devemos diferenciar uma função\n",
    "\n",
    "* `params`, utilizado no treinamento visto anteriormente, define quais parâmetros serão considerados para a diferenciação.\n",
    "\n",
    "* `gradient` calcula o gradiente de uma função, em relação aos parâmetros dados, via diferenciação automática.\n",
    "\n",
    "* `gradient` usa `backwards propagation` (veja o uso de `pullback(f, args...)` na sua definição).\n",
    "\n",
    "* **Observação:** Podemos \"pular\" o `Flux.train!` e escrever métodos de otimização com outros métodos de diferenciação, como `ForwardDiff` (forwards propagation) e `ReverseDiff` (backward mas mais genérico e não tão eficiente quando o `gradient` no caso de redes neurais em questão). Leia mais sobre isso e sobre outros pacotes em [JuliaDiff](https://juliadiff.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Atalho para uma sequência de treinos\n",
    "\n",
    "* É comum vermos o uso de \n",
    "\n",
    "```julia\n",
    "@epochs N train!(...)\n",
    "```\n",
    "\n",
    "* Isso é apenas um atalho para um loop com a exibição de época a cada iteração:\n",
    "\n",
    "```julia\n",
    "for i=1:N\n",
    "    train!(...)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "* O parâmetro de `callback` pode ser muito útil para\n",
    "    * exibir informações sobre um *loop* de treinamento\n",
    "    * afetar o treinamento de alguma forma\n",
    "        * interromper quando um determinada acurácia é alcançada;\n",
    "        * interromper quando um limite de iterações o de tempo é alcançado;\n",
    "        * trocar os parâmetros correntes de busca caso o treinamente não esteja indo com muito sucesso;\n",
    "        * substituir, acrescentar ou retirar determinados parâmetros do processo de treinamento;\n",
    "        * validar o treinamento de acordo com outra amostra de dados;\n",
    "        * etc.\n",
    "\n",
    "* Vale lembrar que `callbacks` estão presentes em outros pacotes também, como `DifferentialEquations`, etc., para executar alguma instrução no meio de algum outro processo (e.g. resolução de uma EDO).\n",
    "\n",
    "* O *Callback* é uma *keyword*, ou seja, é preciso passar como `cb = funcao_de_callback`, exceto que, no Julia 1.6, se a própria função tiver o nome da *keyword*, i.e. `cb`, no caso, então podemos passá-la direto como `Flux.train!(loss, ps, data, opt; cb)`.\n",
    "\n",
    "* Veja mais em [Callback Helpers](https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métodos de otimização\n",
    "\n",
    "* Uma das opções, em `Flux.train!(loss, ps, data, opt, cb)` é o método de otimização `opt`.\n",
    "\n",
    "* Não há opção *default* para o método de otimização. É preciso escolher um.\n",
    "\n",
    "* Há várias opções.\n",
    "\n",
    "* Todos os métodos estão implementados como diferentes \"despachos\" da função `Flux.Optimise.apply!`, em [src/optimise/optimisers.jl](https://github.com/FluxML/Flux.jl/blob/master/src/optimise/optimisers.jl).\n",
    "\n",
    "* Essa função `apply!` é chamada no processo de atualização do passo, dentro da função [update!](https://github.com/FluxML/Flux.jl/blob/master/src/optimise/train.jl#L26), após o gradiente ter sido calculado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 19 methods for generic function <b>apply!</b>:<ul><li> apply!(o::<b>ClipNorm</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:666</a></li> <li> apply!(o::<b>ClipValue</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:655</a></li> <li> apply!(o::<b>AMSGrad</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:415</a></li> <li> apply!(o::<b>RMSProp</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:139</a></li> <li> apply!(o::<b>ADAGrad</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:346</a></li> <li> apply!(o::<b>RADAM</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:213</a></li> <li> apply!(o::<b>ADADelta</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:377</a></li> <li> apply!(o::<b>WeightDecay</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:641</a></li> <li> apply!(o::<b>ExpDecay</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:617</a></li> <li> apply!(o::<b>ADAM</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:172</a></li> <li> apply!(o::<b>AdaBelief</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:521</a></li> <li> apply!(o::<b>Flux.Optimise.Optimiser</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:551</a></li> <li> apply!(o::<b>Nesterov</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:102</a></li> <li> apply!(o::<b>AdaMax</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:262</a></li> <li> apply!(o::<b>InvDecay</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:577</a></li> <li> apply!(o::<b>Descent</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:39</a></li> <li> apply!(o::<b>NADAM</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:455</a></li> <li> apply!(o::<b>Momentum</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:69</a></li> <li> apply!(o::<b>OADAM</b>, x, Δ) in Flux.Optimise at <a href=\"file:///Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl\" target=\"_blank\">/Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:304</a></li> </ul>"
      ],
      "text/plain": [
       "# 19 methods for generic function \"apply!\":\n",
       "[1] apply!(o::ClipNorm, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:666\n",
       "[2] apply!(o::ClipValue, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:655\n",
       "[3] apply!(o::AMSGrad, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:415\n",
       "[4] apply!(o::RMSProp, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:139\n",
       "[5] apply!(o::ADAGrad, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:346\n",
       "[6] apply!(o::RADAM, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:213\n",
       "[7] apply!(o::ADADelta, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:377\n",
       "[8] apply!(o::WeightDecay, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:641\n",
       "[9] apply!(o::ExpDecay, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:617\n",
       "[10] apply!(o::ADAM, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:172\n",
       "[11] apply!(o::AdaBelief, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:521\n",
       "[12] apply!(o::Flux.Optimise.Optimiser, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:551\n",
       "[13] apply!(o::Nesterov, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:102\n",
       "[14] apply!(o::AdaMax, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:262\n",
       "[15] apply!(o::InvDecay, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:577\n",
       "[16] apply!(o::Descent, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:39\n",
       "[17] apply!(o::NADAM, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:455\n",
       "[18] apply!(o::Momentum, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:69\n",
       "[19] apply!(o::OADAM, x, Δ) in Flux.Optimise at /Users/rrosa/.julia/packages/Flux/qp1gc/src/optimise/optimisers.jl:304"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(Flux.Optimise.apply!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando em lotes o gradiente estocástico\n",
    "\n",
    "* No caderno anterior, fizemos exemplos com passos \"tradicionais\", ou seja, levando-se em consideração todos os dados da amostra em cada \"época\".\n",
    "\n",
    "* O método de gradiente estocástico utiliza apenas um dado da amostra em cada passo, escolhido aleatoriamente, completando uma \"época\" quando todos os dados são utilizados (no caso sem reposição) ou com algumas reutilizações e outros esquecios (no caso com reposição).\n",
    "\n",
    "* E um caso intermediário é o **em lotes** *(batch)*, em que a amostra é dividida em grupos iguais escolhidos aleatoriamente e todos de um mesmo grupo são considerados a cada passo.\n",
    "\n",
    "* Essa metodologia pode ser aplicada, na verdade, em conjunto com qualquer método de otimização, não apenas o de gradiente descendente.\n",
    "\n",
    "* Para implementar isso, podemos fazer essa seleção manualmente ou usar a função "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização das camadas\n",
    "\n",
    "* A inicialização das camadas é um ponto importante do processo.\n",
    "\n",
    "* Isso é particularmente importante em camadas convolucioais ou recorrentes.\n",
    "\n",
    "* Por *default*, `Flux.jl` inicializa essas camadas com o método `Flux.glorot_uniform`.\n",
    "\n",
    "* Outros métodos estão disponíveis: `glorot_normal`, `kalman_uniform`, `kalman_normal`.\n",
    "\n",
    "* Mais sobre isso em [Layer Initialization](https://fluxml.ai/Flux.jl/stable/utilities/#Layer-Initialization-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--NAVIGATOR-->\n",
    "\n",
    "---\n",
    "[<- 5. Treinando redes neurais](05.00-Treinando_redes_neurais.ipynb) | [Página inicial](00.00-Pagina_inicial.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
